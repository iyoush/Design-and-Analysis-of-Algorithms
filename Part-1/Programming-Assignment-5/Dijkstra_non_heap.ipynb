{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(r'/home/cloudera/Desktop/educational/ADA1/Programming Assignment 5/dijkstraData.txt')\n",
    "G = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lay out the adjacency list representation of the graph as a python dict\n",
    "for line in f:\n",
    "\ts = line.split()[0]\n",
    "\tv_list = line.split()[1:]\n",
    "\tG[s] = v_list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the set V that stores all vertices\n",
    "V = set(G.keys())\n",
    "# create the dict that stores the shortest paths to each node from the node '1'\n",
    "A = {}\n",
    "for k in G.keys():\n",
    "\tA[k] = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print G\n",
    "#print A\n",
    "#print V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to represent the Graph as a list of edges (with the corresponding edge-lengths) in order to facilitate Dijkstra's algorithm. We can do this using a map-only hadoop job. Create a file named **mapper.py** with the below code:\n",
    "\n",
    "    import sys\n",
    "    for line in sys.stdin:\n",
    "        key = line.split()[0]\n",
    "        value = line.split()[1:]\n",
    "        for v in value:\n",
    "            print '%s,%s' %(key, v)\n",
    "            \n",
    "Then run the below commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [cloudera@localhost Programming Assignment 5]$ hadoop fs -put dijkstraData.txt\n",
    "    [cloudera@localhost Programming Assignment 5]$ hadoop jar $STREAMING -input dijkstraData.txt -output Prog_ass_5 -mapper \"python mapper.py\" -file mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    packageJobJar: [mapper.py, /tmp/hadoop-cloudera/hadoop-unjar2341889022427608157/] [] /tmp/streamjob5268864320777423389.jar tmpDir=null\n",
    "    15/08/08 19:47:59 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
    "    15/08/08 19:47:59 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
    "    15/08/08 19:48:00 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-cloudera/mapred/local]\n",
    "    15/08/08 19:48:00 INFO streaming.StreamJob: Running job: job_201508081707_0001\n",
    "    15/08/08 19:48:00 INFO streaming.StreamJob: To kill this job, run:\n",
    "    15/08/08 19:48:00 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost.localdomain:8021 -kill job_201508081707_0001\n",
    "    15/08/08 19:48:00 INFO streaming.StreamJob: Tracking URL: http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201508081707_0001\n",
    "    15/08/08 19:48:01 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
    "    15/08/08 19:48:13 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
    "    15/08/08 19:48:18 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
    "    15/08/08 19:48:21 INFO streaming.StreamJob: Job complete: job_201508081707_0001\n",
    "    15/08/08 19:48:21 INFO streaming.StreamJob: Output: Prog_ass_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job was successful. Now we need to look into the output directory to make sure that the output is as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [cloudera@localhost Programming Assignment 5]$ hadoop fs -ls Prog_ass_5\n",
    "    Found 3 items\n",
    "    -rw-r--r--   3 cloudera cloudera          0 2015-08-08 19:48 Prog_ass_5/_SUCCESS\n",
    "    drwxr-xr-x   - cloudera cloudera          0 2015-08-08 19:48 Prog_ass_5/_logs\n",
    "    -rw-r--r--   3 cloudera cloudera         36 2015-08-08 19:48 Prog_ass_5/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure that the output file is in the format we expect, lets run the below command:\n",
    "    [cloudera@localhost Programming Assignment 5]$ hadoop fs -cat Prog_ass_5/part*\n",
    "    1,2,3\t\n",
    "    1,3,3\t\n",
    "    2,3,1\t\n",
    "    2,4,2\t\n",
    "    3,4,50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good. So, lets get the output file from the HDFS onto the local file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [cloudera@localhost Programming Assignment 5]$ hadoop fs -cat Prog_ass_5/part* > edgelist.txt\n",
    "    [cloudera@localhost Programming Assignment 5]$ cat edgelist.txt\n",
    "    1,2,3\t\n",
    "    1,3,3\t\n",
    "    2,3,1\t\n",
    "    2,4,2\t\n",
    "    3,4,50\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the edge list\n",
    "E = {}\n",
    "i = 0\n",
    "f = open(r'/home/cloudera/Desktop/educational/ADA1/Programming Assignment 5/edgelist.txt')\n",
    "for line in f:\n",
    "    data = line.strip().split(',')\n",
    "    E[i] = {'Node1':data[0], 'Node2':data[1], 'Length':data[2]}\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start Dijkstra's algorithm\n",
    "# Initialize X and A\n",
    "X = set(['1'])\n",
    "A['1'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The main while loop\n",
    "while X<>V:\n",
    "    # for each edge that has tail in X and head in V\n",
    "    l = []\n",
    "    for e in E.keys():\n",
    "        if (E[e]['Node1'] in X) and (E[e]['Node2'] not in X):\n",
    "            l.append((E[e]['Node2'], A[E[e]['Node1']] + int(E[e]['Length'])))\n",
    "\n",
    "    N2, A2 = min(l, key = lambda x:x[1])\n",
    "    X.add(N2)\n",
    "    A[N2] = A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2505 3068\n"
     ]
    }
   ],
   "source": [
    "#print X\n",
    "#print V\n",
    "print A['188'], A['197']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The shortest path from node '0' to node '188' is 2505. The shortest path from node '0' to node '197' is 3068."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
