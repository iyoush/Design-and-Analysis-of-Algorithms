{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Design and Analysis of Algorithms I\n",
    "##Programming Assignment 4\n",
    "###Computing strongly connected components (SCCs) in a directed acyclical graph.\n",
    "\n",
    "This problem involves finding the five biggest SCCs in a directed acyclical graph. The graph is made available in the form of a 70MB text file where each row represents one edge, with the left entry representing the \"from\" node and the right entry representing the \"to\" node.\n",
    "\n",
    "Before solving the actual problem (where the given graph is a 70MB file), I have created a small test \n",
    "graph (shown below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [cloudera@localhost Programming Assignment 4]$ head SCC.txt\n",
    "\n",
    "    1 1 \n",
    "    1 2 \n",
    "    1 5 \n",
    "    1 6 \n",
    "    1 7 \n",
    "    1 3 \n",
    "    1 8 \n",
    "    1 4 \n",
    "    2 47646 \n",
    "    2 47647"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Pre-processing:\n",
    "We would want to do three things before we can start implementing the algorithm itself. \n",
    "\n",
    "1) First we must notice that any sink nodes (i.e. nodes that don't have any edges leaving them) will not have a row in the above representation of the graph. This will cause problems in the correct implementation of the algorithm. Therefore we need to include these sink nodes (if any) in our graph. We can achieve this by appending \"ghost edges\" (i.e. edges that have a sink node as their \"from\" node but have no \"to\" node associated with it) at the end of the above graph.\n",
    "\n",
    "2) we would want to change the representation of the graph to an adjasency list representation. This will give us huge savings on storage. This won't be apparent in this timny graph, but the difference will become clear on the original graph. \n",
    "\n",
    "3) we would (naively) want to create a new graph that is the reverse of the test graph. There are more efficient ways of doing this, but we will start off with a simplistic approach. \n",
    "\n",
    "We will write map reduce jobs on Hadoop for points 2) and 3).\n",
    "#####Step 1: adding ghost edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v1 = set()\n",
    "v2 = set()\n",
    "\n",
    "f = open(r'/home/cloudera/Desktop/educational/ADA1/Programming Assignment 4/SCC.txt')\n",
    "for line in f:\n",
    "\ttail = line.strip().split()[0]\n",
    "\thead = line.strip().split()[1]\n",
    "\tv1.add(tail)\n",
    "\tv2.add(head)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add v2 - v1 to the graph\n",
    "f = open(r'/home/cloudera/Desktop/educational/ADA1/Programming Assignment 4/SCC.txt', 'a')\n",
    "for v in (v2- v1):\n",
    "    f.write(v + ' x' + '\\n')\n",
    "for v in (v1- v2):\n",
    "    f.write('x '+ v + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Step 2: changing the graph representation to adjacency list\n",
    "Create a mapper file **adj_list_mapper.py** with the below code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    import sys\n",
    "    for line in sys.stdin:\n",
    "        try:\n",
    "\n",
    "            data = line.strip().split()\n",
    "            key, value = data\n",
    "            print '%s\\t%s' % (key, value)\n",
    "        except ValueError:\n",
    "            sys.stderr.write(\"%s\\n\" % line)\n",
    "            exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a reducer file **adj_list_reducer.py** with the below code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    import sys\n",
    "\n",
    "    nodelist = \"\"\n",
    "    oldkey = None\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        data =  line.strip().split(\"\\t\")\n",
    "\n",
    "        if len(data)!= 2:\n",
    "            continue\n",
    "        thiskey, thisnode = data\n",
    "\n",
    "        if oldkey and oldkey != thiskey:\n",
    "            print \"%s,%s\" % (oldkey, nodelist)\n",
    "\n",
    "            nodelist = \"\"\n",
    "        oldkey = thiskey\n",
    "        if nodelist == \"\":\n",
    "            nodelist = thisnode\n",
    "        else:\n",
    "            nodelist = nodelist+','+thisnode\n",
    "\n",
    "    if oldkey !=None:\n",
    "        print \"%s,%s\" % (oldkey, nodelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [cloudera@localhost Programming Assignment 4]$ hadoop fs -put SCC.txt\n",
    "    [cloudera@localhost Programming Assignment 4]$ hadoop fs -ls\n",
    "\n",
    "    -rw-r--r--   3 cloudera cloudera   75321172 2015-08-01 15:32 SCC.txt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [cloudera@localhost Programming Assignment 4]$ hadoop jar $STREAMING -input SCC.txt -output adj_list_graph -mapper \"python adj_list_mapper.py\" -reducer \"python adj_list_reducer.py\" -file adj_list_mapper.py -file adj_list_reducer.py\n",
    "\n",
    "    packageJobJar: [adj_list_mapper.py, adj_list_reducer.py, /tmp/hadoop-cloudera/hadoop-unjar5038415595277101068/] [] /tmp/streamjob4092261144970766109.jar tmpDir=null\n",
    "    15/07/30 21:50:31 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n",
    "    15/07/30 21:50:32 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
    "    15/07/30 21:50:32 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-cloudera/mapred/local]\n",
    "    15/07/30 21:50:32 INFO streaming.StreamJob: Running job: job_201507302114_0001\n",
    "    15/07/30 21:50:32 INFO streaming.StreamJob: To kill this job, run:\n",
    "    15/07/30 21:50:32 INFO streaming.StreamJob: UNDEF/bin/hadoop job  -Dmapred.job.tracker=localhost.localdomain:8021 -kill job_201507302114_0001\n",
    "    15/07/30 21:50:32 INFO streaming.StreamJob: Tracking URL: http://0.0.0.0:50030/jobdetails.jsp?jobid=job_201507302114_0001\n",
    "    15/07/30 21:50:33 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
    "    15/07/30 21:50:45 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
    "    15/07/30 21:50:50 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
    "    15/07/30 21:50:53 INFO streaming.StreamJob: Job complete: job_201507302114_0001\n",
    "    15/07/30 21:50:53 INFO streaming.StreamJob: Output: adj_list_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [cloudera@localhost Programming Assignment 4]$ hadoop fs -ls adj_list_graph\n",
    "\n",
    "    Found 3 items\n",
    "    -rw-r--r--   3 cloudera cloudera          0 2015-08-01 15:35 adj_list_graph/_SUCCESS\n",
    "    drwxr-xr-x   - cloudera cloudera          0 2015-08-01 15:33 adj_list_graph/_logs\n",
    "    -rw-r--r--   3 cloudera cloudera   41197060 2015-08-01 15:35 adj_list_graph/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [cloudera@localhost Programming Assignment 4]$ hadoop fs -cat adj_list_graph/part* > SCC_test_adj.txt\n",
    "    [cloudera@localhost Programming Assignment 4]$ head SCC_adj.txt \n",
    "\n",
    "    1,4,8,2,1,5,6,7,3\t\n",
    "    10,23,171168,104816,27,13,12,26,5,30555,1,8,7,32,19,18,30,104784,171171,171170,29,171169,17,24,28\t\n",
    "    100,48,64,46,45,44,43,387408,57,387407,75250,42,41,40,39,112824,47\t\n",
    "    1000,24876,24868,24832,24872,24877,24878,265325,24900,24904,24908,24909,907,24916,24835,24928,24932,24946,24833\t\n",
    "    10000,221257,56930,221262,9971,10013,9978,10014,221263,9999,9998,221256,9883,9882,221255,9846,217444,9872,144710,10015,9982,10009,133589,49255,221261,133588,9952,221260,10007,159023,9939,9849,221259,10006,9928,9925,10004,9922,10003,9918,9916,10002,221258\t\n",
    "    100000,176535\t\n",
    "    100001,x\t\n",
    "    100002,176526,99996,99995,99994,176523,100006,100005,100003,176538,100000,99999\t\n",
    "    100003,176518,176517,99995,99994,176512,176536,100006,176540,176539,176531,176534,176530,100005,100002,176538,176516,100000,99999,176529,176528,176537,176527,176526,176525,176515,176514,176513,176524,176523,99997,176520,176519,99996\t\n",
    "    100004,416294,416292,416293,416304,416303,416302,416301,416300,416299,416298,416297,416296,416295\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Step 3: reversing the original graph\n",
    "Now that we have the test graph in the adjacency list format, lets proceed to reverse it. Create the file **rev_mapper.py** with the below code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    import sys\n",
    "    for line in sys.stdin:\n",
    "        key, value =  line.strip().split()\n",
    "        print '%s\\t%s' % (value, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need to create another reducer. We can reuse the **adj_list_reducer.py** file. Run the below command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [cloudera@localhost Programming Assignment 4]$ hadoop jar $STREAMING -input SCC.txt -output rev_graph -mapper \"python rev_mapper.py\" -reducer \"python adj_list_reducer.py\" -file rev_mapper.py -file adj_list_reducer.py\n",
    "\n",
    "    packageJobJar: [rev_mapper.py, adj_list_reducer.py, /tmp/hadoop-cloudera/hadoop-unjar1949625360006895700/] [] /tmp/streamjob2353632039566807504.jar tmpDir=null\n",
    "    .....................\n",
    "    .....................\n",
    "    15/07/30 22:09:58 INFO streaming.StreamJob:  map 0%  reduce 0%\n",
    "    15/07/30 22:10:07 INFO streaming.StreamJob:  map 100%  reduce 0%\n",
    "    15/07/30 22:10:13 INFO streaming.StreamJob:  map 100%  reduce 100%\n",
    "    15/07/30 22:10:16 INFO streaming.StreamJob: Job complete: job_201507302114_0002\n",
    "    15/07/30 22:10:16 INFO streaming.StreamJob: Output: rev_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [cloudera@localhost Programming Assignment 4]$ hadoop fs -ls rev_graph\n",
    "    Found 3 items\n",
    "    -rw-r--r--   3 cloudera cloudera          0 2015-08-01 15:42 rev_graph/_SUCCESS\n",
    "    drwxr-xr-x   - cloudera cloudera          0 2015-08-01 15:41 rev_graph/_logs\n",
    "    -rw-r--r--   3 cloudera cloudera   42808289 2015-08-01 15:42 rev_graph/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [cloudera@localhost Programming Assignment 4]$ hadoop fs -cat rev_graph/part* > SCC_rev.txt\n",
    "    [cloudera@localhost Programming Assignment 4]$ head SCC_rev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Implementing Kosaraju's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading in the reversed graph\n",
    "f = open(r'/home/cloudera/Desktop/educational/ADA1/Programming Assignment 4/SCC_rev.txt')\n",
    "g_rev = {}\n",
    "for line in f:\n",
    "    tail = line.strip().split(',')[0]\n",
    "    heads = line.strip().split(',')[1:]\n",
    "    g_rev[tail] = heads    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875715\n"
     ]
    }
   ],
   "source": [
    "print len(g_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {}\n",
    "is_first_pass = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DFS_Loop(G):\n",
    "    # define the global variables\n",
    "\n",
    "    # Create a dictionary for book keeping. This dictionary will keep track of: \n",
    "    # - whether a node has been explored or not\n",
    "    # - the finishing times of each node in the first Pass of DFS on reversed graph G_rev\n",
    "    # - the leader node for each node in second pass of DFS on the forward graph G\n",
    "    global t\n",
    "    t = 0\n",
    "    global S \n",
    "    S = ''\n",
    "    if is_first_pass == 1:\n",
    "        #create the dict for book keeping\n",
    "        for key in G.keys():\n",
    "            d[key] = {'explored':0, 't':0, 'leader':''}\n",
    "        \n",
    "        # In the first pass the order of picking the nodes doesn't matter\n",
    "        sort_key = None\n",
    "    else:\n",
    "        # mark all nodes as unexplored\n",
    "        for key in G.keys():\n",
    "            d[key]['explored']= 0\n",
    "        # in the second pass, the nodes must be picked in the reverse order of their finishing times 't' \n",
    "        sort_key = lambda x: d[x]['t']\n",
    "        \n",
    "    for i in sorted(G.keys(), key = sort_key, reverse = True):\n",
    "        if i == 'x':\n",
    "            continue\n",
    "        else:\n",
    "            if d[i]['explored'] <> 1:\n",
    "                S = i\n",
    "                DFS(G,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DFS(G, i):\n",
    "\n",
    "    # mark i as explored\n",
    "    d[i]['explored'] = 1\n",
    "    # set leader of i as node S\n",
    "    #global S\n",
    "    d[i]['leader'] = S\n",
    "    for j in G[i]:\n",
    "        if j == 'x': # We have reached a ghost node; meaning 'i' is a sink node. No further recursion to be done.\n",
    "            continue\n",
    "        else:\n",
    "            if d[j]['explored'] <>1:\n",
    "                DFS(G,j)\n",
    "\n",
    "    global t\n",
    "    t = t + 1\n",
    "    d[i]['t'] = t\n",
    "    #print 'Node: ', i, 't: ', t, 'S: ', S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import resource, sys\n",
    "resource.setrlimit(resource.RLIMIT_STACK, (2**29,-1))\n",
    "sys.setrecursionlimit(10**6)\n",
    "\n",
    "DFS_Loop(g_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875715\n"
     ]
    }
   ],
   "source": [
    "print len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_first_pass = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the forward graph for the 2nd pass of DFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [cloudera@localhost Programming Assignment 4]$ cat SCC_test.txt\n",
    "\n",
    "    1,4\n",
    "    2,8\n",
    "    3,6\n",
    "    4,7\n",
    "    5,2\n",
    "    6,9\n",
    "    7,1\n",
    "    8,6,5\n",
    "    9,7,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading in the original graph\n",
    "f = open(r'/home/cloudera/Desktop/educational/ADA1/Programming Assignment 4/SCC_adj.txt')\n",
    "g = {}\n",
    "for line in f:\n",
    "    tail = line.strip().split(',')[0]\n",
    "    heads = line.strip().split(',')[1:]\n",
    "    g[tail] = heads  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DFS_Loop(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(d, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(r'/home/cloudera/Desktop/educational/ADA1/Programming Assignment 4/final_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [cloudera@localhost Programming Assignment 4]$ cat final_df.csv | cut -d, -f3 | sort | uniq -c | sort -g -r > leaders.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [cloudera@localhost Programming Assignment 4]$ head leaders.txt\n",
    "\n",
    "     434821 99999\n",
    "        968 14448\n",
    "        459 69681\n",
    "        313 33604\n",
    "        211 97132\n",
    "        205 828413\n",
    "        197 525409\n",
    "        177 448227\n",
    "        162 842535\n",
    "        152 747580"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
